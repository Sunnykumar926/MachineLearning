
------------------------------------------------------- XGBOOST REGRESSOR ----------------------------------------------------------------------------------------------------------------

def tune_xgboost_hyperparameters(train_df, target_col, feature_cols, n_trials=20):
    def objective(trial):
        params = {
            "max_depth": trial.suggest_int("max_depth", 3, 10),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.1, log=True),
            "min_child_weight": trial.suggest_int("min_child_weight", 1, 100),
            "subsample": trial.suggest_float("subsample", 0.5, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
            "n_estimators": trial.suggest_int("n_estimators", 500, 1500),
            "reg_alpha": trial.suggest_float("reg_alpha", 0, 1),
            "reg_lambda": trial.suggest_float("reg_lambda", 0, 1),
            "tree_method": "gpu_hist",  # Use GPU for XGBoost
            "gpu_id": 0  # Specify GPU device
        }

        cv = KFold(n_splits=5, shuffle=True, random_state=42)
        cv_scores = []
        for train_idx, val_idx in cv.split(train_df):
            X_train_cv = train_df.iloc[train_idx][feature_cols]
            y_train_cv = train_df.iloc[train_idx][target_col]
            X_val_cv = train_df.iloc[val_idx][feature_cols]
            y_val_cv = train_df.iloc[val_idx][target_col]

            model = XGBRegressor(enable_categorical=True, random_state=42, **params)
            model.fit(X_train_cv, y_train_cv, eval_set=[(X_val_cv, y_val_cv)], eval_metric="rmse", early_stopping_rounds=50, verbose=False)
            preds = model.predict(X_val_cv, iteration_range=(0, model.best_iteration + 1))
            rmse = np.sqrt(mean_squared_error(y_val_cv, preds))
            cv_scores.append(rmse)
        return np.mean(cv_scores)

    study = optuna.create_study(direction="minimize")
    study.optimize(objective, n_trials=n_trials)
    return study.best_trial.params

# Tune XGBoost hyperparameters
xgboost_best_params = tune_xgboost_hyperparameters(preprocessed_train, target_column, all_feature_columns)
print("Best XGBoost Hyperparameters:", xgboost_best_params)

------------------------------------------------------ CATBOOST REGRESSOR -------------------------------------------------------------------------------------------------------------------

def tune_catboost_hyperparameters(train_df, target_col, feature_cols, n_trials=20):
    def objective(trial):
        params = {
            "iterations": trial.suggest_int("iterations", 500, 1500),
            "depth": trial.suggest_int("depth", 3, 10),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.1, log=True),
            "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 0, 1),
            "random_strength": trial.suggest_float("random_strength", 0, 1),
            "bagging_temperature": trial.suggest_float("bagging_temperature", 0, 1),
            "border_count": trial.suggest_int("border_count", 32, 255),
            "verbose": False,
            "task_type": "GPU",  # Use GPU for CatBoost
            "devices": "0"  # Specify GPU device
        }

        cv = KFold(n_splits=5, shuffle=True, random_state=42)
        cv_scores = []
        for train_idx, val_idx in cv.split(train_df):
            X_train_cv = train_df.iloc[train_idx][feature_cols]
            y_train_cv = train_df.iloc[train_idx][target_col]
            X_val_cv = train_df.iloc[val_idx][feature_cols]
            y_val_cv = train_df.iloc[val_idx][target_col]

            model = CatBoostRegressor(**params)
            model.fit(X_train_cv, y_train_cv, eval_set=(X_val_cv, y_val_cv), early_stopping_rounds=50, verbose=False)
            preds = model.predict(X_val_cv)
            rmse = np.sqrt(mean_squared_error(y_val_cv, preds))
            cv_scores.append(rmse)
        return np.mean(cv_scores)

    study = optuna.create_study(direction="minimize")
    study.optimize(objective, n_trials=n_trials)
    return study.best_trial.params

# Tune CatBoost hyperparameters
catboost_best_params = tune_catboost_hyperparameters(preprocessed_train, target_column, all_feature_columns)
print("Best CatBoost Hyperparameters:", catboost_best_params)
